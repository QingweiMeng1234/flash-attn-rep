{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch-lamb\n",
      "  Downloading pytorch_lamb-1.0.0-py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: tqdm in /home/m/.local/lib/python3.10/site-packages (from pytorch-lamb) (4.66.1)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/m/.local/lib/python3.10/site-packages (from pytorch-lamb) (2.1.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (2023.12.2)\n",
      "Requirement already satisfied: filelock in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (4.9.0)\n",
      "Requirement already satisfied: jinja2 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/m/.local/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-lamb) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/m/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch-lamb) (12.3.101)\n",
      "Requirement already satisfied: packaging in /home/m/.local/lib/python3.10/site-packages (from tensorboardX->pytorch-lamb) (23.2)\n",
      "Requirement already satisfied: numpy in /home/m/.local/lib/python3.10/site-packages (from tensorboardX->pytorch-lamb) (1.26.2)\n",
      "Collecting protobuf>=3.20\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 KB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/m/.local/lib/python3.10/site-packages (from torchvision->pytorch-lamb) (10.1.0)\n",
      "Requirement already satisfied: requests in /home/m/.local/lib/python3.10/site-packages (from torchvision->pytorch-lamb) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/m/.local/lib/python3.10/site-packages (from jinja2->torch>=0.4.1->pytorch-lamb) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/m/.local/lib/python3.10/site-packages (from requests->torchvision->pytorch-lamb) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/m/.local/lib/python3.10/site-packages (from requests->torchvision->pytorch-lamb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/m/.local/lib/python3.10/site-packages (from requests->torchvision->pytorch-lamb) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/m/.local/lib/python3.10/site-packages (from requests->torchvision->pytorch-lamb) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/m/.local/lib/python3.10/site-packages (from sympy->torch>=0.4.1->pytorch-lamb) (1.3.0)\n",
      "Installing collected packages: protobuf, tensorboardX, torchvision, pytorch-lamb\n",
      "Successfully installed protobuf-4.25.1 pytorch-lamb-1.0.0 tensorboardX-2.6.2.2 torchvision-0.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing max_length: 128\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "Epoch 1/30, Average Loss: 0.9488\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "torch.Size([11, 12, 128, 64])\n",
      "Epoch 2/30, Average Loss: 0.8757\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 229\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    228\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    230\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    231\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 141\u001b[0m, in \u001b[0;36mTextFolderDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths[idx], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    140\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 141\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mids[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoding\u001b[38;5;241m.\u001b[39mids[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]))\n\u001b[1;32m    143\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoding\u001b[38;5;241m.\u001b[39mids[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoding\u001b[38;5;241m.\u001b[39mids[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertModel,AdamW\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from glob import glob\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# Assuming you have 'flash_attn_interface' properly installed and imported\n",
    "from flash_attn_interface import flash_attention\n",
    "torch.manual_seed(0)\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        head_dim = config.hidden_size // self.num_attention_heads\n",
    "\n",
    "        # Define linear layers for q, k, v transformations\n",
    "        self.query = nn.Linear(config.hidden_size, self.num_attention_heads * head_dim)\n",
    "        self.key = nn.Linear(config.hidden_size, self.num_attention_heads * head_dim)\n",
    "        self.value = nn.Linear(config.hidden_size, self.num_attention_heads * head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        if hidden_states.dim() == 2:\n",
    "            # Add a sequence length dimension if it's missing\n",
    "            hidden_states = hidden_states.unsqueeze(1)\n",
    "        # print(f\"Hidden States shape is {hidden_states.shape}\")\n",
    "        batch_size, seq_length, hidden_size = hidden_states.size()\n",
    "        assert hidden_size % self.num_attention_heads == 0, \"Hidden size must be divisible by the number of attention heads\"\n",
    "\n",
    "        # Apply linear transformations to get q, k, v\n",
    "        q = self.query(hidden_states).view(batch_size, seq_length, self.num_attention_heads, -1)\n",
    "        k = self.key(hidden_states).view(batch_size, seq_length, self.num_attention_heads, -1)\n",
    "        v = self.value(hidden_states).view(batch_size, seq_length, self.num_attention_heads, -1)\n",
    "\n",
    "        q, k, v = [x.permute(0, 2, 1, 3).to(torch.float16) for x in (q, k, v)]\n",
    "        # Call flash_attention\n",
    "        print(q.shape)\n",
    "        flash_output = flash_attention(q, k, v, dropout_prob=0.0, causal=False)\n",
    "        flash_output = flash_output.to(torch.float32)\n",
    "        # Reshape and return the new hidden state\n",
    "        flash_output = flash_output.permute(0, 2, 1, 3).contiguous()\n",
    "        new_hidden_state = flash_output.view(batch_size, seq_length, hidden_size)\n",
    "\n",
    "        return new_hidden_state.to(torch.float32)\n",
    "\n",
    "class CustomBertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertSelfAttention(config)\n",
    "        self.attention_output_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_activation = nn.GELU()\n",
    "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.output_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None, \n",
    "                encoder_hidden_states=None, encoder_attention_mask=None, \n",
    "                past_key_values=None, output_attentions=False):\n",
    "        # Attention block\n",
    "        # print(\"Hidden states shape in CustomBertLayer:\", hidden_states.shape)\n",
    "        # print(\"Attention Starts\")\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        # print(\"Attention Ends\")\n",
    "        attention_output = self.attention_output_norm(attention_output + hidden_states)  # Add residual connection\n",
    "    \n",
    "        # Intermediate and output block (Feed-forward)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        intermediate_output = self.intermediate_activation(intermediate_output)\n",
    "        layer_output = self.output(intermediate_output)\n",
    "        layer_output = self.output_norm(layer_output + attention_output)  # Add residual connection\n",
    "        layer_output = layer_output.repeat(11, 1, 1, 1)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "\n",
    "class CustomBertModel(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace default BertLayer with CustomBertLayer in the encoder\n",
    "        self.encoder.layer = nn.ModuleList([CustomBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        # Add a linear layer to project hidden states to vocabulary size\n",
    "        self.cls = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_ids.size(), input_ids.device)\n",
    "        head_mask = self.get_head_mask(None, self.config.num_hidden_layers)\n",
    "\n",
    "        # Get embeddings from the BERT embeddings layer\n",
    "        embedding_output = self.embeddings(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        # Forward pass through the encoder layers\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # The last hidden state from the encoder output\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        # Project hidden states to vocabulary size\n",
    "        logits = self.cls(sequence_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Function to get current memory usage\n",
    "def get_gpu_memory_usage():\n",
    "    # Returns the current GPU memory usage in MB\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "    cached = torch.cuda.memory_reserved() / (1024 * 1024)\n",
    "    return allocated, cached\n",
    "\n",
    "class TextFolderDataset(Dataset):\n",
    "    def __init__(self, file_directory, file_pattern, tokenizer, max_length):\n",
    "        self.filepaths = glob(os.path.join(file_directory, file_pattern))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.filepaths[idx], 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        input_ids = encoding.ids[:self.max_length] + [0] * (self.max_length - len(encoding.ids[:self.max_length]))\n",
    "        attention_mask = [1] * len(encoding.ids[:self.max_length]) + [0] * (self.max_length - len(encoding.ids[:self.max_length]))\n",
    "        \n",
    "        # Create labels (for masked language modeling, you might mask some tokens here)\n",
    "        labels = input_ids[:]  # In practice, apply masking strategy here\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)  # Add labels\n",
    "        }\n",
    "\n",
    "# Function to train a WordPiece tokenizer\n",
    "def train_tokenizer(file_directory, file_pattern):\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=30522,\n",
    "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "    files = glob(os.path.join(file_directory, file_pattern))\n",
    "    tokenizer.train(files, trainer)\n",
    "    return tokenizer\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy of our predictions vs labels.\n",
    "    It flattens both the predictions and labels arrays to compare them element-wise.\n",
    "    \"\"\"\n",
    "    # Convert the highest logit to predicted label (argmax over the last dimension)\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    \n",
    "    # Flatten the true labels array\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = np.sum(pred_flat == labels_flat)\n",
    "\n",
    "    # Calculate accuracy as the ratio of correct predictions to total predictions\n",
    "    accuracy = correct_predictions / len(labels_flat)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Your training and evaluation code\n",
    "max_lengths = [128, 256, 512, 768, 1024,1200,1300,1400,1900,2300,2800]\n",
    "training_times = []\n",
    "memory_usages = []\n",
    "\n",
    "file_directory = \"openwebtext/\"\n",
    "file_pattern = \"urlsf_subset01-32*\"\n",
    "eval_file_pattern = 'urlsf_subset01-33*'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 30\n",
    "for max_length in max_lengths:\n",
    "    # Your existing tokenizer training, dataset creation, etc.\n",
    "    print(\"Processing max_length:\", max_length)\n",
    "    tokenizer = train_tokenizer(file_directory, file_pattern)\n",
    "    tokenizer.save(f\"model/tokenizer_bert_flash_attn_{max_length}.json\")\n",
    "    tokenizer = Tokenizer.from_file(f\"model/tokenizer_bert_flash_attn_{max_length}.json\")\n",
    "    dataset = TextFolderDataset(file_directory, file_pattern, tokenizer, max_length)\n",
    "    eval_dataset = TextFolderDataset(file_directory, eval_file_pattern, tokenizer, max_length)\n",
    "    data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    eval_data_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n",
    "    config = BertConfig(\n",
    "        vocab_size=tokenizer.get_vocab_size(),\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        max_position_embeddings=max_length,\n",
    "    )\n",
    "\n",
    "    custom_model = CustomBertModel(config).to(torch.device(\"cuda\"))\n",
    "    # custom_model = custom_model.half()  # Convert model's parameters and buffers to half precision\n",
    "\n",
    "    optimizer = AdamW(custom_model.parameters(), lr=5e-5)\n",
    "    start_time = time.time()\n",
    "    start_allocated, start_cached = get_gpu_memory_usage()\n",
    "\n",
    "    # Training loop (simplified)\n",
    "    custom_model.train()\n",
    "    scaler = GradScaler()\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            attention_mask = batch['attention_mask'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            labels = batch['labels'].to(torch.device(\"cuda\"), dtype=torch.long)  # Shape: [batch_size, seq_length]\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                logits = custom_model(input_ids=input_ids, attention_mask=attention_mask)  # Shape: [batch_size, seq_length, num_classes]\n",
    "    \n",
    "                # Flatten logits to shape [batch_size * seq_length, num_classes]\n",
    "                logits_flat = logits.view(-1, logits.size(-1))\n",
    "        \n",
    "                # Flatten labels to shape [batch_size * seq_length], keep as torch.long\n",
    "                labels_flat = labels.view(-1)\n",
    "        \n",
    "                loss = loss_fn(logits_flat, labels_flat)\n",
    "            # print(\" The type for the loss is \")\n",
    "            # print(loss.dtype)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / input_ids.shape[0]\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        if avg_loss < 0.5:\n",
    "            break\n",
    " \n",
    "    end_time = time.time()\n",
    "    end_allocated, end_cached = get_gpu_memory_usage()\n",
    "    delta_cached = end_cached - start_cached\n",
    "    training_times.append(end_time - start_time)\n",
    "    memory_usages.append(delta_cached)\n",
    "    del custom_model, optimizer, tokenizer, dataset, eval_dataset, data_loader, eval_data_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(max_lengths, training_times, marker='o')\n",
    "plt.title('Training Time vs Max Length')\n",
    "plt.xlabel('Max Length')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(max_lengths, memory_usages, marker='o')\n",
    "plt.title('Memory Usage vs Max Length')\n",
    "plt.xlabel('Max Length')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57.50393986701965, 77.59242367744446, 91.95445609092712, 96.26931309700012, 104.74510741233826]\n"
     ]
    }
   ],
   "source": [
    "print(training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2330.0, 1908.0, 3972.0, 5146.0, 6852.0]\n"
     ]
    }
   ],
   "source": [
    "print(memory_usages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing max_length: 1900\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/m/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:942: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Average Loss: 0.9566\n",
      "Epoch 2/30, Average Loss: 0.8937\n",
      "Epoch 3/30, Average Loss: 0.8429\n",
      "Epoch 4/30, Average Loss: 0.8030\n",
      "Epoch 5/30, Average Loss: 0.7703\n",
      "Epoch 6/30, Average Loss: 0.7439\n",
      "Epoch 7/30, Average Loss: 0.7220\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertModel,AdamW\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from glob import glob\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# Assuming you have 'flash_attn_interface' properly installed and imported\n",
    "from flash_attn_interface import flash_attention\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        head_dim = config.hidden_size // self.num_attention_heads\n",
    "\n",
    "        # Define linear layers for q, k, v transformations\n",
    "        self.query = nn.Linear(config.hidden_size, self.num_attention_heads * head_dim)\n",
    "        self.key = nn.Linear(config.hidden_size, self.num_attention_heads * head_dim)\n",
    "        self.value = nn.Linear(config.hidden_size, self.num_attention_heads * head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        if hidden_states.dim() == 2:\n",
    "            # Add a sequence length dimension if it's missing\n",
    "            hidden_states = hidden_states.unsqueeze(1)\n",
    "        # print(f\"Hidden States shape is {hidden_states.shape}\")\n",
    "        batch_size, seq_length, hidden_size = hidden_states.size()\n",
    "        assert hidden_size % self.num_attention_heads == 0, \"Hidden size must be divisible by the number of attention heads\"\n",
    "\n",
    "        # Apply linear transformations to get q, k, v\n",
    "        q = self.query(hidden_states).view(batch_size, seq_length, self.num_attention_heads, -1)\n",
    "        k = self.key(hidden_states).view(batch_size, seq_length, self.num_attention_heads, -1)\n",
    "        v = self.value(hidden_states).view(batch_size, seq_length, self.num_attention_heads, -1)\n",
    "\n",
    "        q, k, v = [x.permute(0, 2, 1, 3).to(torch.float16) for x in (q, k, v)]\n",
    "        # Call flash_attention\n",
    "        flash_output = flash_attention(q, k, v, dropout_prob=0.0, causal=False)\n",
    "        flash_output = flash_output.to(torch.float32)\n",
    "        # Reshape and return the new hidden state\n",
    "        flash_output = flash_output.permute(0, 2, 1, 3).contiguous()\n",
    "        new_hidden_state = flash_output.view(batch_size, seq_length, hidden_size)\n",
    "\n",
    "        return new_hidden_state.to(torch.float32)\n",
    "\n",
    "class CustomBertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertSelfAttention(config)\n",
    "        self.attention_output_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_activation = nn.GELU()\n",
    "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.output_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None, \n",
    "                encoder_hidden_states=None, encoder_attention_mask=None, \n",
    "                past_key_values=None, output_attentions=False):\n",
    "        # Attention block\n",
    "        # print(\"Hidden states shape in CustomBertLayer:\", hidden_states.shape)\n",
    "        # print(\"Attention Starts\")\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        # print(\"Attention Ends\")\n",
    "        attention_output = self.attention_output_norm(attention_output + hidden_states)  # Add residual connection\n",
    "    \n",
    "        # Intermediate and output block (Feed-forward)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        intermediate_output = self.intermediate_activation(intermediate_output)\n",
    "        layer_output = self.output(intermediate_output)\n",
    "        layer_output = self.output_norm(layer_output + attention_output)  # Add residual connection\n",
    "        layer_output = layer_output.repeat(11, 1, 1, 1)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "\n",
    "class CustomBertModel(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace default BertLayer with CustomBertLayer in the encoder\n",
    "        self.encoder.layer = nn.ModuleList([CustomBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        # Add a linear layer to project hidden states to vocabulary size\n",
    "        self.cls = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_ids.size(), input_ids.device)\n",
    "        head_mask = self.get_head_mask(None, self.config.num_hidden_layers)\n",
    "\n",
    "        # Get embeddings from the BERT embeddings layer\n",
    "        embedding_output = self.embeddings(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        # Forward pass through the encoder layers\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # The last hidden state from the encoder output\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        # Project hidden states to vocabulary size\n",
    "        logits = self.cls(sequence_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Function to get current memory usage\n",
    "def get_gpu_memory_usage():\n",
    "    # Returns the current GPU memory usage in MB\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "    cached = torch.cuda.memory_reserved() / (1024 * 1024)\n",
    "    return allocated, cached\n",
    "\n",
    "class TextFolderDataset(Dataset):\n",
    "    def __init__(self, file_directory, file_pattern, tokenizer, max_length):\n",
    "        self.filepaths = glob(os.path.join(file_directory, file_pattern))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.filepaths[idx], 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        input_ids = encoding.ids[:self.max_length] + [0] * (self.max_length - len(encoding.ids[:self.max_length]))\n",
    "        attention_mask = [1] * len(encoding.ids[:self.max_length]) + [0] * (self.max_length - len(encoding.ids[:self.max_length]))\n",
    "        \n",
    "        # Create labels (for masked language modeling, you might mask some tokens here)\n",
    "        labels = input_ids[:]  # In practice, apply masking strategy here\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)  # Add labels\n",
    "        }\n",
    "\n",
    "# Function to train a WordPiece tokenizer\n",
    "def train_tokenizer(file_directory, file_pattern):\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=30522,\n",
    "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "    files = glob(os.path.join(file_directory, file_pattern))\n",
    "    tokenizer.train(files, trainer)\n",
    "    return tokenizer\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy of our predictions vs labels.\n",
    "    It flattens both the predictions and labels arrays to compare them element-wise.\n",
    "    \"\"\"\n",
    "    # Convert the highest logit to predicted label (argmax over the last dimension)\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    \n",
    "    # Flatten the true labels array\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = np.sum(pred_flat == labels_flat)\n",
    "\n",
    "    # Calculate accuracy as the ratio of correct predictions to total predictions\n",
    "    accuracy = correct_predictions / len(labels_flat)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Your training and evaluation code\n",
    "max_lengths = [1900,2300,2800]\n",
    "training_times = []\n",
    "memory_usages = []\n",
    "\n",
    "file_directory = \"openwebtext/\"\n",
    "file_pattern = \"urlsf_subset01-32*\"\n",
    "eval_file_pattern = 'urlsf_subset01-33*'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 30\n",
    "for max_length in max_lengths:\n",
    "    # Your existing tokenizer training, dataset creation, etc.\n",
    "    print(\"Processing max_length:\", max_length)\n",
    "    tokenizer = train_tokenizer(file_directory, file_pattern)\n",
    "    tokenizer.save(f\"model/tokenizer_bert_flash_attn_{max_length}.json\")\n",
    "    tokenizer = Tokenizer.from_file(f\"model/tokenizer_bert_flash_attn_{max_length}.json\")\n",
    "    dataset = TextFolderDataset(file_directory, file_pattern, tokenizer, max_length)\n",
    "    eval_dataset = TextFolderDataset(file_directory, eval_file_pattern, tokenizer, max_length)\n",
    "    data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    eval_data_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n",
    "    config = BertConfig(\n",
    "        vocab_size=tokenizer.get_vocab_size(),\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        max_position_embeddings=max_length,\n",
    "    )\n",
    "\n",
    "    custom_model = CustomBertModel(config).to(torch.device(\"cuda\"))\n",
    "    # custom_model = custom_model.half()  # Convert model's parameters and buffers to half precision\n",
    "\n",
    "    optimizer = AdamW(custom_model.parameters(), lr=5e-5)\n",
    "    start_time = time.time()\n",
    "    start_allocated, start_cached = get_gpu_memory_usage()\n",
    "\n",
    "    # Training loop (simplified)\n",
    "    custom_model.train()\n",
    "    scaler = GradScaler()\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            attention_mask = batch['attention_mask'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            labels = batch['labels'].to(torch.device(\"cuda\"), dtype=torch.long)  # Shape: [batch_size, seq_length]\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                logits = custom_model(input_ids=input_ids, attention_mask=attention_mask)  # Shape: [batch_size, seq_length, num_classes]\n",
    "    \n",
    "                # Flatten logits to shape [batch_size * seq_length, num_classes]\n",
    "                logits_flat = logits.view(-1, logits.size(-1))\n",
    "        \n",
    "                # Flatten labels to shape [batch_size * seq_length], keep as torch.long\n",
    "                labels_flat = labels.view(-1)\n",
    "        \n",
    "                loss = loss_fn(logits_flat, labels_flat)\n",
    "            # print(\" The type for the loss is \")\n",
    "            # print(loss.dtype)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / input_ids.shape[0]\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        if avg_loss < 0.5:\n",
    "            break\n",
    " \n",
    "    end_time = time.time()\n",
    "    end_allocated, end_cached = get_gpu_memory_usage()\n",
    "    delta_cached = end_cached - start_cached\n",
    "    training_times.append(end_time - start_time)\n",
    "    memory_usages.append(delta_cached)\n",
    "    del custom_model, optimizer, tokenizer, dataset, eval_dataset, data_loader, eval_data_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(max_lengths, training_times, marker='o')\n",
    "plt.title('Training Time vs Max Length')\n",
    "plt.xlabel('Max Length')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(max_lengths, memory_usages, marker='o')\n",
    "plt.title('Memory Usage vs Max Length')\n",
    "plt.xlabel('Max Length')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_usages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
