{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115c9945-0748-418e-b1f3-0cd6f31ebc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  2%|█▋                                                                                  | 1/50 [00:26<21:22, 26.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Average Loss: 19.2852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                                | 2/50 [00:48<19:12, 24.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Average Loss: 13.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████                                                                               | 3/50 [01:11<18:17, 23.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Average Loss: 7.5801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▋                                                                             | 4/50 [01:33<17:38, 23.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Average Loss: 6.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▍                                                                           | 5/50 [01:56<17:08, 22.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Average Loss: 5.9575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████                                                                          | 6/50 [02:18<16:37, 22.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Average Loss: 5.9379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▊                                                                        | 7/50 [02:41<16:12, 22.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Average Loss: 5.8414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████▍                                                                      | 8/50 [03:03<15:51, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Average Loss: 5.7599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████                                                                     | 9/50 [03:26<15:25, 22.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Average Loss: 5.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 10/50 [03:48<14:57, 22.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Average Loss: 5.6318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████▎                                                                | 11/50 [04:10<14:33, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Average Loss: 5.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▉                                                               | 12/50 [04:33<14:12, 22.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Average Loss: 5.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████████████████▌                                                             | 13/50 [04:55<13:50, 22.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Average Loss: 5.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████▏                                                           | 14/50 [05:17<13:25, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Average Loss: 5.3676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 15/50 [05:40<13:03, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Average Loss: 5.3104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▌                                                        | 16/50 [06:02<12:43, 22.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Average Loss: 5.2441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████▏                                                      | 17/50 [06:25<12:21, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Average Loss: 5.1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████████▉                                                     | 18/50 [06:47<11:58, 22.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Average Loss: 5.1238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████▌                                                   | 19/50 [07:09<11:32, 22.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Average Loss: 5.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 20/50 [07:32<11:13, 22.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Average Loss: 5.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████▊                                                | 21/50 [07:55<10:53, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Average Loss: 4.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████▌                                              | 22/50 [08:18<10:32, 22.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Average Loss: 4.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████▏                                            | 23/50 [08:40<10:09, 22.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Average Loss: 4.8510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████▊                                           | 24/50 [09:03<09:47, 22.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Average Loss: 4.8028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 25/50 [09:26<09:25, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Average Loss: 4.7303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████▏                                       | 26/50 [09:48<09:04, 22.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Average Loss: 15.4651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████▊                                      | 27/50 [10:11<08:42, 22.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Average Loss: 4.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████▍                                    | 28/50 [10:34<08:20, 22.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Average Loss: 4.2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████▏                                  | 29/50 [10:57<07:57, 22.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Average Loss: 4.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 30/50 [11:19<07:35, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Average Loss: 3.8031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████▍                               | 31/50 [11:42<07:09, 22.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Average Loss: 3.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████                              | 32/50 [12:04<06:46, 22.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Average Loss: 3.5743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████████████████████████████████▊                            | 33/50 [12:27<06:23, 22.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Average Loss: 3.5211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████▍                          | 34/50 [12:49<06:01, 22.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Average Loss: 3.4405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 35/50 [13:13<05:41, 22.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Average Loss: 3.3907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████▊                       | 36/50 [13:35<05:18, 22.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Average Loss: 3.3493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████████████████████████████████████████▍                     | 37/50 [13:58<04:57, 22.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Average Loss: 3.2985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████                    | 38/50 [14:21<04:34, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Average Loss: 3.2724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████████████████████████████████████▋                  | 39/50 [14:45<04:12, 22.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Average Loss: 3.2332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 40/50 [15:08<03:50, 23.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Average Loss: 3.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████████████               | 41/50 [15:31<03:27, 23.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Average Loss: 3.1735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████████▋             | 42/50 [15:54<03:03, 22.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Average Loss: 3.1463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████████▍           | 43/50 [16:16<02:40, 22.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Average Loss: 3.1223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████████          | 44/50 [16:39<02:16, 22.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Average Loss: 3.1007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 45/50 [17:02<01:54, 22.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Average Loss: 3.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████████████████████▎      | 46/50 [17:25<01:31, 22.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Average Loss: 3.0640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████████████████     | 47/50 [17:47<01:07, 22.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Average Loss: 3.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████▋   | 48/50 [18:09<00:45, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Average Loss: 3.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████▎ | 49/50 [18:31<00:22, 22.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Average Loss: 3.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [18:53<00:00, 22.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Average Loss: 3.0106\n",
      "The time needed for training is 1133.9099576473236 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from flash_attn_interface import flash_attention\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import AdamW\n",
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "torch.manual_seed(0)\n",
    "class FlashAttentionLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.n_embd / config.num_attention_heads)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_length, hidden_size = hidden_states.size()\n",
    "\n",
    "        # Apply linear transformations and reshape for multi-head attention\n",
    "        q = self.query(hidden_states).view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "        k = self.key(hidden_states).view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "        v = self.value(hidden_states).view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "\n",
    "        # Transpose to get the shape (batch_size, num_heads, seq_length, head_size)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Call flash_attention\n",
    "        attention_output = flash_attention(q, k, v, dropout_prob=0.0, causal=False)\n",
    "        # Reshape attention output back to (batch_size, seq_length, hidden_size)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, hidden_size)\n",
    "        return (attention_output,)\n",
    "\n",
    "class CustomGPT2Block(GPT2Block):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace the standard attention with FlashAttentionLayer\n",
    "        self.attn = FlashAttentionLayer(config)\n",
    "\n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, output_attentions=False):\n",
    "        # Attention block\n",
    "        attn_outputs = self.attn(hidden_states, attention_mask)\n",
    "        a = attn_outputs[0]  # Output of the flash attention\n",
    "\n",
    "        # Feed Forward block\n",
    "        mlp_output = self.mlp(self.ln_2(a))\n",
    "        hidden_states = mlp_output + a\n",
    "\n",
    "        outputs = (hidden_states,) + attn_outputs[1:]  # Add attention outputs if they are present\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class CustomGPT2Model(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace standard GPT2 blocks with CustomGPT2Block\n",
    "        self.h = nn.ModuleList([CustomGPT2Block(config) for _ in range(config.n_layer)])\n",
    "        # Initialize the embedding layers\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        default_layer_norm_eps = 1e-5\n",
    "\n",
    "        # Initialize the final layer normalization with the default epsilon\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd, eps=default_layer_norm_eps)\n",
    "\n",
    "        # Initialize the language modeling head\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        # Prepare inputs to the model\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        batch_size = input_ids.shape[0]\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            device = input_ids.device\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            device = inputs_embeds.device\n",
    "\n",
    "        if position_ids is None:\n",
    "            # Create default position_ids if None provided\n",
    "            position_ids = torch.arange(0, input_shape[-1], dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(batch_size, input_shape[-1]).to(input_ids.device)\n",
    "\n",
    "        # Get embeddings from GPT2Model\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "        for block in self.h:\n",
    "            outputs = block(hidden_states, attention_mask=attention_mask)\n",
    "            hidden_states = outputs[0]\n",
    "\n",
    "        # Final layer normalization and head\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Retrieve the correct vocabulary size from the tokenizer\n",
    "correct_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Update your GPT2 configuration with the correct vocabulary size\n",
    "config = GPT2Config(\n",
    "    vocab_size=correct_vocab_size,\n",
    "    n_positions=1024,\n",
    "    n_ctx=1024,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12\n",
    ")\n",
    "trad_model = GPT2LMHeadModel(config)\n",
    "# Initialize the custom model with Flash Attention\n",
    "custom_model = CustomGPT2Model(config)\n",
    "traditional_params = set(trad_model.state_dict().keys())\n",
    "custom_params = set(custom_model.state_dict().keys())\n",
    "\n",
    "# Find common parameters\n",
    "common_params = traditional_params & custom_params\n",
    "\n",
    "# Copy only common parameters\n",
    "common_state_dict = {name: trad_model.state_dict()[name] for name in common_params}\n",
    "custom_model.load_state_dict(common_state_dict, strict=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Transfer the custom_model to the GPU (if available)\n",
    "custom_model.to(device)\n",
    "\n",
    "# Define your TextFolderDataset\n",
    "# Define your TextFolderDataset\n",
    "class TextFolderDataset(Dataset):\n",
    "    def __init__(self, file_directory, file_pattern, max_length):\n",
    "        self.filepaths = glob(os.path.join(file_directory, file_pattern))\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad_token\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.filepaths[idx], 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'][0]\n",
    "        attention_mask = encoding['attention_mask'][0]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Create your dataset and data loader\n",
    "file_directory = \"openwebtext/\"\n",
    "file_pattern = \"urlsf_subset01-3[2,3]*\"\n",
    "max_length = 512\n",
    "dataset = TextFolderDataset(file_directory, file_pattern, max_length)\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(custom_model.parameters(), lr=5e-5)\n",
    "accumulation_steps = 2\n",
    "num_epochs = 50\n",
    "\n",
    "start_time = time.time()\n",
    "scaler = GradScaler()\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Transfer the data to the GPU\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            logits = custom_model(inputs)\n",
    "            # Reshape logits and labels for CrossEntropyLoss\n",
    "            # logits shape: [batch_size, seq_length, vocab_size]\n",
    "            # labels shape: [batch_size, seq_length]\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (len(train_dataloader) / accumulation_steps)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if avg_loss < 1.5:\n",
    "        break\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"The time needed for training is {end_time - start_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22660eef-9bab-418a-9fe2-7e08325a1536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
